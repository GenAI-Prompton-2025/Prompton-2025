from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import chromedriver_autoinstaller
import ssl
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

# bs4 ê¸°ë³¸ ì„¤ì •
headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}

def get_soup(url, headers):
    # ìš”ì²­ ë° BeautifulSoup ê°ì²´ ìƒì„±
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup

# âœ… **ëŒ“ê¸€ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜**
def get_comments_from_page(comment_list):
    # ë‚ ì§œì™€ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    comments_data = []
    for comment in comment_list:
        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        comment_text_element = comment.select_one('.comment-list__text-override')
        comment_text = comment_text_element.get_text(strip=True) if comment_text_element else "N/A"

        # ë°ì´í„° ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
        comments_data.append(comment_text)
    return comments_data





ssl._create_default_https_context = ssl._create_unverified_context  # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”

# ìë™ìœ¼ë¡œ ChromeDriver ì„¤ì¹˜
chromedriver_autoinstaller.install()

# Selenium ì‹¤í–‰ ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Chrome WebDriver ì‹¤í–‰
service = Service()
driver = webdriver.Chrome(service=service, options=chrome_options)

# ëª©ë¡ í˜ì´ì§€ URL
base_url = "https://www.albamon.com/alba-talk/experience?pageIndex={page}&searchKeyword=&sortType=CREATED_DATE"

# ë²„íŠ¼ í´ë¦­ í›„ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
def crawl_experience(endPageIndex):
    results = []
    for page in range(1, endPageIndex+1):  
        print(f"ğŸš€ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹œì‘...")
        driver.get(base_url.format(page=page))  # í˜ì´ì§€ ì´ë™
        time.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    
        # âœ… ë²„íŠ¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ê²Œì‹œê¸€ ëª©ë¡)
        # buttons = driver.find_elements(By.CSS_SELECTOR, ".Button_button__S9rjD.Button_text__5x_Cn.Button_large___Kecx.tertiary")

         # ë²„íŠ¼ì´ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        try:
            buttons = WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".Button_button__S9rjD.Button_text__5x_Cn.Button_large___Kecx.tertiary"))                )
        except:
            print(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            continue  # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        
        for i in range(len(buttons)):  # ê° ë²„íŠ¼ í´ë¦­
            if i >= len(buttons):
                print(f"âš ï¸ {i}ë²ˆì§¸ ë²„íŠ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ ê¸¸ì´: {len(buttons)}")
                break

            # JavaScriptë¥¼ ì´ìš©í•´ í´ë¦­ (í´ë¦­ ì˜¤ë¥˜ ë°©ì§€)
            driver.execute_script("arguments[0].scrollIntoView();", buttons[i])
            driver.execute_script("arguments[0].click();", buttons[i])  # JavaScript í´ë¦­ ì‚¬ìš©

            time.sleep(2)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°

            # í˜„ì¬ í˜ì´ì§€ URLì—ì„œ talkNo ì¶”ì¶œ
            current_url = driver.current_url
            talk_no = current_url.split("/")[-1].split("?")[0]  # talkNo ì¶”ì¶œ
            print(f"ğŸ”¹ ê²Œì‹œê¸€ URL: {current_url}")

            # âœ… BeautifulSoupìœ¼ë¡œ í˜ì´ì§€ íŒŒì‹±
            soup = get_soup(current_url, headers)

            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë‚´ìš©, ë‚ ì§œ í¬ë¡¤ë§
            try:
                # ì½˜í…ì¸  ì˜ì—­ (ê²Œì‹œê¸€ ì œëª©, ë‚´ìš©, ì‘ì„±ì¼ì)
                title_element = soup.select_one('.DetailTitle_detail__header--title__Bbp40')
                contents_element = soup.select_one('.Detail_content__content__hJ5M7')
                date_element = soup.select_one('.CommonInfos_info__wrapper__aGcEl > div:nth-child(2)')
                view_count_element = soup.select_one('.experience__span--view')

                title = title_element.get_text(strip=True) if title_element else "N/A"
                contents = contents_element.get_text(strip=True) if contents_element else "N/A"
                date = date_element.get_text(strip=True) if date_element else "N/A"
                view_count = view_count_element.get_text(strip=True) if date_element else "N/A"

                # í•„ìˆ˜ê°’ì´ ì—†ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì €ì¥í•˜ì§€ ì•Šê³  ë„˜ì–´ê°€ê¸°
                if not title_element or not title_element.get_text(strip=True) or not contents or not date:
                    continue

                # ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
                comment_list = soup.select('.CommentList_comment-contents__YVrtF > ul li')
                parsed_comments = get_comments_from_page(comment_list)
                
                # ë°ì´í„° ì €ì¥
                results.append({
                    "talkNo": talk_no,
                    "Title": title,
                    "Contents": contents,
                    "Date": date,
                    "ViewCount": view_count,
                    "Comments": parsed_comments
                })

                print(f"talkNo: {talk_no}")
                print(f"Title: {title}")
                print(f"Contents: {contents}")
                print(f"Date: {date}")
                print(f"comments : {parsed_comments}")
                print(f"ViewCount: {view_count}")
                print("-" * 80)

            except Exception as e:
                print(f"âŒ ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # ë’¤ë¡œ ê°€ê¸°
            driver.back()
            time.sleep(2)

            # ë‹¤ì‹œ ë²„íŠ¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            buttons = driver.find_elements(By.CSS_SELECTOR, ".Button_button__S9rjD.Button_text__5x_Cn.Button_large___Kecx.tertiary")


    return results

# í¬ë¡¤ë§ ì‹¤í–‰
data = crawl_experience(1330)

# ë¸Œë¼ìš°ì € ì¢…ë£Œ
driver.quit()

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
df = pd.DataFrame(data)
# ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = "crawling_result"
os.makedirs(output_dir, exist_ok=True)

# CSV íŒŒì¼ë¡œ ì €ì¥
csv_file =  os.path.join(output_dir, "crawling_detail_result.csv")
df.to_csv(csv_file, index=False, encoding='utf-8-sig')  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥
print(f"Data saved to {csv_file}")